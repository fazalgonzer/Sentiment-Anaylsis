{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "%pwd\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from box.exceptions import BoxValueError\n",
    "import yaml\n",
    "\n",
    "import json\n",
    "import joblib\n",
    "from ensure import ensure_annotations\n",
    "from box import ConfigBox\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "import base64\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directories(path_to_dir:list,verbose=True):\n",
    "    for path in path_to_dir:\n",
    "        os.makedirs(path,exist_ok=True)\n",
    "       # if verbose:\n",
    "           # logger.info(f\"created dierctor ast {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_yaml(path_to_yaml:Path)-> ConfigBox:\n",
    "    try:\n",
    "        with open(path_to_yaml) as yaml_file:\n",
    "            content=yaml.safe_load(yaml_file)\n",
    "           # logger.info(f\"yaml file {path_to_yaml} load sucessfuly\")\n",
    "            return ConfigBox(content)\n",
    "    except BoxValueError:\n",
    "        raise ValueError(\"yaml file is empty\")\n",
    "    except Exception as e:\n",
    "        raise(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "@dataclass\n",
    "class Datatrainingconfig:\n",
    "    root_dir:Path\n",
    "    source_name: str\n",
    "    transformed_path: Path\n",
    "    saving_model: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManger:\n",
    "    def __init__(self,config_file_path,params_file_path):\n",
    "        self.config=read_yaml(config_file_path)\n",
    "        self.params=read_yaml(params_file_path)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_data_training_config(self) -> Datatrainingconfig:\n",
    "        config=self.config.data_training\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "        data_training_config=Datatrainingconfig(\n",
    "\n",
    "\n",
    "            root_dir=config.root_dir,\n",
    "\n",
    "\n",
    "            \n",
    "            source_name= config.source_name,\n",
    "           \n",
    "           \n",
    "            transformed_path=config.transformed_path,\n",
    "            saving_model=config.saving_model\n",
    "\n",
    "                \n",
    "        )\n",
    "      \n",
    "        return data_training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk , DatasetDict\n",
    "from transformers import AutoModelForSequenceClassification , Trainer, TrainingArguments, DataCollatorWithPadding ,AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Data_training:\n",
    "    def __init__(self, config:Datatrainingconfig):\n",
    "        self.config=config \n",
    "        self.dataset=load_from_disk(self.config.transformed_path)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(self.config.source_name,num_labels=5)\n",
    "        \n",
    "    def training_args(self):\n",
    "        \n",
    "        tokenized_datasets=self.dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "        train_testvalid = self.dataset['train'].train_test_split(test_size=0.1)\n",
    "        train_test = train_testvalid['train'].train_test_split(test_size=0.1)\n",
    "        tokenized_datasets = DatasetDict({\n",
    "        'train': train_test['train'],\n",
    "        'test': train_testvalid['test'],\n",
    "        'valid': train_test['test']\n",
    "})\n",
    "        train_dataset = tokenized_datasets['train']\n",
    "        valid_dataset = tokenized_datasets['valid']\n",
    "        test_dataset = tokenized_datasets['test']  \n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.config.source_name) \n",
    "\n",
    "        data_collator = DataCollatorWithPadding(tokenizer)\n",
    "        training_args = TrainingArguments(\n",
    "        output_dir='./results',          # output directory\n",
    "        num_train_epochs=3,              # number of training epochs\n",
    "        per_device_train_batch_size=8,   # batch size for training\n",
    "        per_device_eval_batch_size=8,    # batch size for evaluation\n",
    "        warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "        weight_decay=0.01,               # strength of weight decay\n",
    "        logging_dir='./logs',            # directory for storing logs\n",
    "        logging_steps=10,\n",
    "        evaluation_strategy='epoch'      # Evaluate every epoch\n",
    "    )\n",
    "        trainer = Trainer(\n",
    "        model=self.model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "        args=training_args,                  # training arguments, defined above\n",
    "        train_dataset=train_dataset,         # training dataset\n",
    "        eval_dataset=valid_dataset,          # evaluation dataset\n",
    "        data_collator=data_collator          # data collator\n",
    "    )\n",
    "        trainer.train()\n",
    "\n",
    "        validation_results = trainer.evaluate(eval_dataset=valid_dataset)\n",
    "        print(\"Validation results:\", validation_results)\n",
    "\n",
    "        # Evaluate the model on the test set\n",
    "        test_results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "        print(\"Test results:\", test_results)\n",
    "\n",
    "        model_save_path = \"./\"\n",
    "        self.model.save_pretrained(self.config.saving_model)\n",
    "        tokenizer.save_pretrained(self.config.saving_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/87501 [00:14<110:11:40,  4.53s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m     datatraining_config\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget_data_training_config()\n\u001b[0;32m      4\u001b[0m     data_training\u001b[38;5;241m=\u001b[39mData_training(config\u001b[38;5;241m=\u001b[39mdatatraining_config)\n\u001b[1;32m----> 7\u001b[0m     \u001b[43mdata_training\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "Cell \u001b[1;32mIn[26], line 41\u001b[0m, in \u001b[0;36mData_training.training_args\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     23\u001b[0m     training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m     24\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./results\u001b[39m\u001b[38;5;124m'\u001b[39m,          \u001b[38;5;66;03m# output directory\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,              \u001b[38;5;66;03m# number of training epochs\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     32\u001b[0m     evaluation_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m      \u001b[38;5;66;03m# Evaluate every epoch\u001b[39;00m\n\u001b[0;32m     33\u001b[0m )\n\u001b[0;32m     34\u001b[0m     trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     35\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,                         \u001b[38;5;66;03m# the instantiated ðŸ¤— Transformers model to be trained\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,                  \u001b[38;5;66;03m# training arguments, defined above\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     39\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator          \u001b[38;5;66;03m# data collator\u001b[39;00m\n\u001b[0;32m     40\u001b[0m )\n\u001b[1;32m---> 41\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m     validation_results \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate(eval_dataset\u001b[38;5;241m=\u001b[39mvalid_dataset)\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation results:\u001b[39m\u001b[38;5;124m\"\u001b[39m, validation_results)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\trainer.py:1859\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1857\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1858\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1860\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1861\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1864\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\trainer.py:2273\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler, torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mReduceLROnPlateau):\n\u001b[0;32m   2271\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m-> 2273\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2274\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2275\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n",
      "File \u001b[1;32md:\\anaconda\\envs\\fazal\\lib\\site-packages\\torch\\nn\\modules\\module.py:2351\u001b[0m, in \u001b[0;36mModule.zero_grad\u001b[1;34m(self, set_to_none)\u001b[0m\n\u001b[0;32m   2349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2350\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m set_to_none:\n\u001b[1;32m-> 2351\u001b[0m         p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2352\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2353\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mgrad_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config=ConfigurationManger(config_file_path=Path(\"config.yaml\"),params_file_path=Path(\"params.yaml\"))\n",
    "    datatraining_config=config.get_data_training_config()\n",
    "    data_training=Data_training(config=datatraining_config)\n",
    "\n",
    "    \n",
    "    data_training.training_args()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fazal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
